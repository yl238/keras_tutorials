{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN - Deep convolutional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schematically, a GAN looks like:\n",
    "1. A `generator` network maps vectors of shape `(latent_dim,)` to images of shape `(32, 32, 3)`.\n",
    "2. A `discriminator` network maps images of shape `(32, 32, 3)` to a binary score estimating the probability that the image is real.\n",
    "3. A `gan` network that chains the generator and discriminator together: `gan(x) = discriminator(generator(x))`. Thus this `gan` network maps latent space vectors to the discriminator's assessment of the realism of these latent vectors as decoded by the generator.\n",
    "4. You train the discriminator using examples of real and fake images along with 'real'/'fake' labels, just as you train any regular image-classification model.\n",
    "5. To train the generator, you use the gradients of the generator's weights with regard to the loss of the `gan` model. This means, at every step, you move the weights of the generator in a direction that makes the discriminator more likely to classify as 'real' the images decoded by the generator. In other words, you train the generator to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)  # Upsample to 32 x 32\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The discriminator\n",
    "Develop a `discriminator` model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes: \"generated image\" or \"real image that comes from the training set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x) # One dropout layer (important trick)\n",
    "\n",
    "x = layers.Dense(1, activation='sigmoid')(x) # Classification layer\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimintator_optimizer = keras.optimizers.RMSprop(lr=0.0008,\n",
    "                                                  clipvalue=1.0,\n",
    "                                                  decay=1e-8)\n",
    "discriminator.compile(optimizer=discrimintator_optimizer,\n",
    "                     loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adversarial network\n",
    "The GAN chains the generator and discriminator. \n",
    "- Set the discriminator to be frozen during training: its weights won't be updated when training `gan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train your DCGAN\n",
    "For each epoch, do the following:\n",
    "1. Draw random points in the **latent space** (random noise)\n",
    "2. Generate images with `generator` using this random noise.\n",
    "3. Mix the generated images with real ones.\n",
    "4. Train `discriminator` using these mixed images, with corresponding targets: either 'real' (for the real images) or 'fake' (for the generated images'\n",
    "5. Draw new random points in the latent space\n",
    "6. Train `gan` using these random vectors, with targets that all say 'these are real images'. This updates the weights of the generator (only because the discriminator is frozen inside `gan`) to move them toward getting the discriminator to predict 'these are real images' for generated images: this trains the generator to fool the disciminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train[y_train.flatten() == 6] # Select frog images (class 6)\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0],) + \n",
    "                         (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = 'gan_data' # Where to save generated images\n",
    "\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # sample random points in the latent space\n",
    "    \n",
    "    generated_images = generator.predict(random_latent_vectors) # decode them to fake images\n",
    "    \n",
    "    stop = start + batch_size                 # combines them with real images\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    \n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), # Assembling labels, discriminating real from fake images\n",
    "                            np.zeros((batch_size, 1))])\n",
    "    \n",
    "    labels += 0.05 * np.random.random(labels.shape) # Adds random noise to the labels, AN IMPORTANT Trick!\n",
    "    \n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels) # trains the discriminator\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # samples random points in the latent space\n",
    "    \n",
    "    misleading_targets = np.zeros((batch_size, 1)) # Assembles labels that say 'these are all real images' (it is a lie!)\n",
    "    \n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets) # Trains the generator (via the gan model, where the discriminator weights are frozen)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "   \n",
    "    if step % 100 == 0: # Occasionally saves and plots (every 100 steps)\n",
    "        gan.save_weights('gan.h5') # saves model weights\n",
    "        \n",
    "        print('discriminator loss:', d_loss) # print metrics\n",
    "        print('adversarial loss:', a_loss)\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0]*255., scale=False) # Saves one generated image\n",
    "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0]*255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png')) # saves one real image for comparison\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "* A GAN consists of a generator network coupled with a discriminator network. The discriminator is trained to differentiate between the output of the generator and real images from a training dataset, and the generator is trained to fool the discriminator. Remarkably, the generator never sees images from the training set directly; the information it has about the data comes from the discriminator.\n",
    "* GANs are difficult to train, because training a GAN is a dynamic process rather than a simple gradient descent process with a fixed loss landscape. Getting a GAN to train correctly requires using a number of heuristic tricks, as well as extensive tuning.\n",
    "* GANs can potentially produce highly realistic images. But unlike VAEs, the latent space they learn doesn't have a neat continuous structure and thus may not be suited for certain practical applications, such as image editing via latent-space concept vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
